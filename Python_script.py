# -*- coding: utf-8 -*-
"""Final_thesis_script

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h1SKhWXxkqknTixqNVM3LyiATF4hqYDa
"""

!pip install torch
!pip install torch_geometric
!pip install openpyxl

from torch_geometric.data import Dataset, Data
import torch
import pandas as pd
import networkx as nx
import os
from collections import Counter
from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
from torch_geometric.data import DataLoader
import torch
from sklearn.model_selection import train_test_split
from torch.utils.data import Subset
from sklearn.model_selection import KFold
import random
import warnings
import itertools
from torch.nn import Linear
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
from torch_geometric.nn import global_mean_pool

# Ignore specific deprecation warnings
warnings.filterwarnings('ignore', category=UserWarning, message=".*'data.DataLoader' is deprecated, use 'loader.DataLoader' instead.*")

# Commented out IPython magic to ensure Python compatibility.
!unzip participants.zip
# %ls

class SocialNetworkDataset(Dataset):
    def __init__(self, graphs, combination, transform=None, pre_transform=None):
        super(SocialNetworkDataset, self).__init__(None, transform, pre_transform)
        self.graphs = graphs
        # self.attributes = attributes  # List of attributes to be used in graph_to_data_object
        self.data_list = [self.convert_to_data(graph, label, combination) for graph, label in graphs.values()]
        self.total_classes = self.calculate_num_classes()

    def convert_to_data(self, graph, label, combination):
        # Pass the attributes list to the graph_to_data_object function
        return graph_to_data_object(graph, label, combination)

    def calculate_num_classes(self):
        # Collect all unique labels
        unique_labels = set()
        for data in self.data_list:
            if data.y.numel() > 1:
                unique_labels.update(data.y.tolist())
            else:
                unique_labels.add(data.y.item())
        return len(unique_labels)

    def len(self):
        return len(self.data_list)

    def get(self, idx):
        return self.data_list[idx]

# CGN model
class GCN(torch.nn.Module):
    def __init__(self, hidden_channels):
        super(GCN, self).__init__()
        torch.manual_seed(12345)
        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        self.conv3 = GCNConv(hidden_channels, hidden_channels)
        self.lin = Linear(hidden_channels, dataset.total_classes)

    def forward(self, x, edge_index, batch):
        # 1. Obtain node embeddings
        x = self.conv1(x, edge_index)
        x = x.relu()
        x = self.conv2(x, edge_index)
        x = x.relu()
        x = self.conv3(x, edge_index)
        x = x.relu()

        # 2. Readout layer
        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]

        # 3. Apply a final classifier
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.lin(x)

        return x

# Reduce the computational time by stopping earlier the testing when there is not improvement in the score.
class EarlyStopping:
    def __init__(self, patience=10, verbose=False, delta=0.001):
        self.patience = patience
        self.verbose = verbose
        self.delta = delta
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_score_max = float('-inf')
        self.early_ind = 0

    def __call__(self, val_f1, model):
        score = val_f1
        self.early_ind += 1
        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_f1, model)
        elif score < self.best_score + self.delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            if score > self.best_score + self.delta:
                self.save_checkpoint(val_f1, model)
                self.best_score = score
                self.counter = 0

    def save_checkpoint(self, val_score, model):
        self.val_score_max = val_score

# Function to generate all combinations of attributes
def attribute_combinations(attributes):
    return [list(comb) for i in range(1, len(attributes)+1) for comb in itertools.combinations(attributes, i)]

# Defining the "age"
def preprocess_age(age_str):
    if age_str == '50+':
        return 50
    elif pd.isna(age_str) or age_str is None:
        return 0
    elif age_str == '18-':
        return 18
    else:
        return int(age_str)


# Defining the "number of children"
def preprocess_num_children(num_children):
    if num_children == 'Expecting first child':
        return 0.5
    elif pd.isna(num_children) or num_children == "I don't know":
        return 0
    elif num_children == 'More than 5':
        return 5
    else:
        return float(num_children)


# Defining the "sex"
def preprocess_sex(sex):
    if sex == 'Female':
        return 1
    else:
        return -1


# Defining the "happiness_child_a"
def preprocess_happiness_child_a(happiness_child_a):
    if happiness_child_a == "I don't know" or happiness_child_a == "[PERSON 1 to 25]'s child has not been born yet":
        return 0
    elif happiness_child_a == "[PERSON 1 to 25]'s happiness in life became less after the birth of the child(ren)" or happiness_child_a == 1 or happiness_child_a == 2:
        return 1
    elif happiness_child_a == "[PERSON 1 to 25]'s happiness in life increased after the birth of the child(ren)" or happiness_child_a == 4 or happiness_child_a == 5:
        return 2
    elif happiness_child_a == "[PERSON 1 to 25]'s happiness in life remained the same after the birth of the child(ren)" or happiness_child_a == 3:
        return 3
    else:
        return happiness_child_a


# Defining the "child_free"
def preprocess_child_free(child_free):
    if child_free == 'Prefers to remain childless':
        return 0
    elif child_free == 'Wishes to have children':
        return 1
    else:
        return 2  # "I donâ€™t know whether person wishes to have children"


# Defining the "friend"
def preprocess_friend(friend):
    if pd.isna(friend) or friend is None:
        return 0
    elif friend == 'Yes, is a friend':
        return 1
    else:
        return -1


# Defining the "relationship type"
def preprocess_relationship_type(relationship):
    if pd.isna(relationship) or relationship is None:
        return 0
    elif isinstance(relationship, str):
        first_number = relationship.split(',')[0].strip()
        return float(first_number)
    else:
        return float(relationship)


# Defining the "f_to_f"
def preprocess_f_to_f(ftf):
    if pd.isna(ftf) or ftf is None:
        return 0
    else:
        return float(ftf)


# Defining the "help"
def preprocess_help(help):
    if pd.isna(help) or help is None:
        return 0
    else:
        return float(help)


# Defining the "non_f2f"
def preprocess_non_f2f(non_f2f):
    if pd.isna(non_f2f) or non_f2f is None:
        return 0
    else:
        return float(non_f2f)


# Covert the dictionary into tha appropriate form for the GNN model
def graph_to_data_object(G, label, features_try):
    y = torch.tensor([label], dtype=torch.long)
    node_mapping = {node: i for i, node in enumerate(G.nodes())}
    edge_list = [(node_mapping[u], node_mapping[v]) for u, v in G.edges()]
    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()

    features = []
    for node, node_data in G.nodes(data=True):  # It converts every node for every network into torch.tensor type.

        node_features = []
        if 'age' in features_try:  # The first check like the 'age' needs to be like the way we've mentioned in the attributes list before.
            node_features.append(preprocess_age(node_data.get('age_a', None)))
        if 'num_children' in features_try:
            node_features.append(preprocess_num_children(node_data.get('num_child_a', 0)))
        if 'sex' in features_try:
            node_features.append(preprocess_sex(node_data.get('sex_a', None)))
        if 'child_free' in features_try:
            node_features.append(preprocess_child_free(node_data.get('childfree_a', None)))
        if 'friend' in features_try:
            node_features.append(preprocess_friend(node_data.get('friend_a', None)))
        if 'happiness' in features_try:
            node_features.append(preprocess_happiness_child_a(node_data.get('happiness_child_a')))
        if 'relationship_type' in features_try:
            node_features.append(preprocess_relationship_type(node_data.get('relation_a')))
        if 'f_to_f' in features_try:
            node_features.append(preprocess_f_to_f(node_data.get('f_to_f')))
        if 'help' in features_try:
            node_features.append(preprocess_help(node_data.get('help')))
        if 'non_f2f' in features_try:
            node_features.append(preprocess_non_f2f(node_data.get('non_f2f')))

        features.append(node_features)

    x = torch.tensor(features, dtype=torch.float)
    return Data(x=x, edge_index=edge_index, y=y)


# Provides the f1-score and confusion matrix of the model
def evaluate(model, loader):
    model.eval()
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for data in loader:
            out = model(data.x, data.edge_index, data.batch)
            _, pred = torch.max(out, dim=1)
            all_preds.extend(pred.cpu().numpy())
            all_labels.extend(data.y.cpu().numpy())
    # Compute F1 score
    validation_f1 = f1_score(all_labels, all_preds, average='macro')
    # Compute the confusion matrix
    conf_matrix = confusion_matrix(all_labels, all_preds)
    return validation_f1, conf_matrix

# We apply oversample process the training data to correct the balance between the classes
def oversample_data(dataset, indices):
    class_count = {}
    for index in indices:
        label = dataset[index].y.item()  # Extract the class label from the dataset at the given index
        class_count[label] = class_count.get(label, 0) + 1

    max_count = max(class_count.values())  # Determine maximum class count to identify the most represented class

    # Create oversampled index list to ensure each class has the same number of samples
    oversampled_indices = []
    for label, count in class_count.items():
        label_indices = [index for index in indices if
                         dataset[index].y.item() == label]
        factor = max_count // count
        oversampled_indices.extend(label_indices * factor + random.sample(label_indices, max_count % count))

    # Shuffle the oversampled_indices to prevent any ordering biases during training
    random.shuffle(oversampled_indices)

    return oversampled_indices

# Read respondents' id
participant_ids = [d for d in os.listdir('participants/') if os.path.isdir(os.path.join('participants/', d))]

# Read the .csv file to load the data
participant_data = pd.read_csv('data_subset.csv')
graphs = {}


# Create a dictionary with a key word the id of participant and the value a list with the first value the respective graph and second the childwish
for pid in participant_ids:
    alters_file_path = os.path.join('participants', pid, 'alters.csv')
    graphml_file_path = os.path.join('participants', pid, 'edgelist.graphml')
    alters_df = pd.read_csv(alters_file_path)
    G = nx.read_graphml(graphml_file_path)

    # Get participant-specific attributes from participant_data
    participant_row = participant_data[participant_data['nomem_encr'] == int(pid)]

    # Create a new node for the participant
    participant_node_id = 'n25'
    G.add_node(participant_node_id)

    # Initialize all node attributes to NA (or suitable defaults)
    na_attributes = {col: None for col in alters_df.columns if col not in ['names_a', 'node_id']}
    G.nodes[participant_node_id].update(na_attributes)

    # Update specific attributes from participant_data
    if not participant_row.empty:

        # Alters to Respondents
        G.nodes[participant_node_id]['has_child_a'] = participant_row['has_child_num'].values[0]
        G.nodes[participant_node_id]['num_child_a'] = participant_row['num_children'].values[0]
        G.nodes[participant_node_id]['sex_a'] = participant_row['sex'].values[0]
        G.nodes[participant_node_id]['age_a'] = participant_row['age'].values[0]
        G.nodes[participant_node_id]['happiness_child_a'] = participant_row['happiness_num'].values[0]
        G.nodes[participant_node_id]['childwish_a'] = participant_row['childwish_num'].values[0]
        G.nodes[participant_node_id]['childfree_a'] = participant_row['has_children'].values[0]
        G.nodes[participant_node_id]['help_child_a'] = participant_row['no_help'].values[0]



    # Connect the new participant node to all other nodes
    for node in G.nodes():
        if node != participant_node_id:
            G.add_edge(participant_node_id, node)

        index = int(node[1:]) + 1
        match = alters_df[alters_df['names_a'] == index]
        if not match.empty:
            attributes = match.to_dict('records')[0]
            for key, value in attributes.items():
                G.nodes[node][key] = value

    graphs[pid] = G

# Ensure that the key column ('nomem_encr' or similar) is a string if necessary
participant_data['nomem_encr'] = participant_data['nomem_encr'].astype(str)

# Create a dictionary to map "childwish" values to numbers
childwish_mapping = {
    "Probably so": 2,
    "Absolutely so": 2,
    "Probably not": 1,
    "Absolutely not": 1
}

# Create a dictionary from the DataFrame for quick lookups
childwish_dict = participant_data.set_index('nomem_encr')['childwish'].to_dict()

# Update each entry in the dictionary
for key in list(graphs.keys()):

    current_graph = graphs[key]
    # Lookup the 'childwish' attribute using the graph's key (convert key to string if necessary)
    childwish_value = childwish_dict.get(str(key), "Default value if not found")
    childwish_number = childwish_mapping.get(childwish_value, 0)  # Default to 2 if not found in mapping
    # Update the dictionary entry to be a list containing the graph and the 'childwish' number
    graphs[key] = [current_graph, childwish_number]

# # CHECK UNIQUE VARIABLES
import pandas as pd
from collections import Counter

# Initialize an empty list to hold all values across all graphs
all_age_values = []
f2f_age_values = []

for key in graphs.keys():

    G_example = graphs[key][0]

    # Create a DataFrame from node attributes
    node_data = {node: G_example.nodes[node] for node in G_example.nodes()}
    node_df = pd.DataFrame.from_dict(node_data, orient='index')

    # Ensure the columns are converted to floats, ignoring non-convertible values
    node_df['age_a'] = pd.to_numeric(node_df['age_a'], errors='coerce')
    node_df['has_child_a'] = node_df['has_child_a'].astype(str)
    node_df['num_child_a'] = node_df['num_child_a'].astype(str)
    node_df['happiness_child_a'] = node_df['happiness_child_a'].astype(str)
    node_df['childwish_a'] = node_df['childwish_a'].astype(str)
    node_df['childfree_a'] = node_df['childfree_a'].astype(str)
    node_df['help_child_a'] = node_df['help_child_a'].astype(str)
    node_df['relationship_type'] = node_df['relation_a'].astype(str)
    node_df['non_f2f'] = node_df['non_f2f'].astype(str)


    # Replace 'nan' with a recognizable string like 'Unknown'
    node_df['relationship_type'] = node_df['relationship_type'].replace('nan', 'Unknown')
    all_age_values.extend(node_df['relationship_type'].tolist())
    f2f_age_values.extend(node_df['non_f2f'].tolist())


# Initialize a counter to hold the number of relevant NA values
na_count = 0

for key in graphs.keys():

    G_example = graphs[key][0]
    additional_data = graphs[key][1]

    # Create a DataFrame from node attributes
    node_data = {node: G_example.nodes[node] for node in G_example.nodes()}
    node_df = pd.DataFrame.from_dict(node_data, orient='index')

    # Convert 'age_a' to numeric, handling non-convertible values by setting them to NaN
    node_df['age_a'] = pd.to_numeric(node_df['age_a'], errors='coerce')

    # Check if the additional data in this graph is NA
    is_additional_data_na = pd.isna(additional_data)
    # Loop through the 'age_a' in node_df to find ages <= 30
    for age in node_df['age_a']:

        if age <= 30 and is_additional_data_na:
            na_count += 1

print(f"Number of NA values in 'graphs[key][1]' when 'age_a' is <= 30: {na_count}")

# Attributes list contain every ego and alter variable that I want to take into account
attributes = ['age', 'num_children', 'sex', 'child_free', 'friend', 'f_to_f', 'relationship_type','help','non_f2f','happiness']
feature_combinations = attribute_combinations(attributes)

k_folds = 5  # Number of folds

# Initialize KFold
kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)

num_combinations = 0
data_dict = {}
dict_combination = {}
idx = 0

# In this way total_comb variable provides as the number of total combinations could happen with the attributes that we use in attribute list
num_of_attributes_in_combination = len(attributes)
total_comb = 0

# For every combination, it returns the right form for the GNN model, so in every try it provides 706 of
# Data(x=[x,x], edge_index=[x, x], y=[x]) for the combination that run in each time for example ['age' and 'sex'] that means that will be Data[x=[26,2],...] for those 2 attributes.
for combination in feature_combinations:

    idx += 1    # Counts the number of combination

    dict_combination[idx] = []  # We create a dictionary that keeps all the information, showing the number of combination, the attributes that used in this combination and the f1-score, in this form {num_combinations:[combination, mean f1-score], num_combinations:[combination, mean f1-score]}
    dict_combination[idx].append(combination)

    dataset = SocialNetworkDataset(graphs, combination)
    torch.manual_seed(12345)    # Shuffling

    dataset_size = len(dataset)
    indices = list(range(dataset_size))

    # Split into training and testing datasets
    train_idxs, test_idxs = train_test_split(indices, test_size=0.2, random_state=42)

    train_dataset = Subset(dataset, train_idxs)
    test_dataset = Subset(dataset, test_idxs)

    # Here is where the cross validation takes place and for every iteration the we apply oversample to the training dataset and not to validation
    for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset), 1):
        early_stopping = EarlyStopping(patience=20, verbose=True)
        train_idexs = oversample_data(train_dataset, train_idx)

        train_subset = Subset(train_dataset, train_idexs)

        validation_subset = Subset(train_dataset, val_idx)

        train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)
        validation_loader = DataLoader(validation_subset, batch_size=64, shuffle=False)

        # Initialize model, criterion, and optimizer
        model = GCN(hidden_channels=64)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
        criterion = torch.nn.CrossEntropyLoss()  # Modify based on

        # Train the model
        for epoch in range(1, 171):

            model.train()
            for data in train_loader:
                optimizer.zero_grad()
                out = model(data.x, data.edge_index, data.batch)
                loss = criterion(out, data.y)
                loss.backward()
                optimizer.step()

            model.eval()
            validation_f1 = evaluate(model, validation_loader)  # Define this function to return F1-score

            # Early stopping
            early_stopping(validation_f1[0], model)
            if early_stopping.early_stop:
                break

        print(f'Fold {fold}, Validation F1 Score: {validation_f1[0]}')

    # Evaluate on the test dataset
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
    final_f1_score, conf_matrix = evaluate(model, test_loader)
    data_dict[idx] = [combination, final_f1_score]

    # Train final_model on all training data here before evaluation
    final_f1_score = evaluate(model, test_loader)
    print(f'\nF1 Score: {final_f1_score[0]}')
    # print(f'\nConfusion Matrix: \n {final_f1_score[1]}\n')
    print(data_dict)
    print(f'\n{idx}/{len(feature_combinations)} combinations have been completed')
    print("\n ---------------------------------------\n \n ")